<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Traffic video captioning &amp; VQA using Large Vision Language Model (LVLM) | Thành Đạt </title> <meta name="author" content="Thành Đạt "> <meta name="description" content="Finetune LVLM to enhance spatial and temporal understanding in traffic video"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img//favicon1/favicon-32x32.png?30edfe67e51b22352bca0ac747dbe60c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tothanhdat2006.github.io/projects/AIC2025/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Thành Đạt</span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Traffic video captioning &amp; VQA using Large Vision Language Model (LVLM)</h1> <p class="post-description">Finetune LVLM to enhance spatial and temporal understanding in traffic video</p> </header> <article> <h2 id="foreword">Foreword</h2> <p>I would like to sincerely thank all of the members in the team, whose effort was poured into this challenge. This result would be achieved without all of you guys: Nguyen Nhu Tinh Anh, Nguyen Tien Huy, Dao Tran Minh Triet, Le Gia Phuc and Vo Lan Tuan.</p> <p>Disclaimer: This post is not an official post for the paper or project, this post is based on my experience and is a reflect of what I learned from this challenge.</p> <h2 id="introduction">Introduction</h2> <p>Official description of AI City Challenge:</p> <blockquote> <p>The AI City Challenge, hosted at ICCV 2025, is designed to push the boundaries of computer vision and AI in real-world settings, driving innovative solutions for smarter transportation, large-scale industrial analytics, and public safety. By tackling multifaceted data sources—from cameras and lidars in roadway intersections to high-fidelity synthetic warehouses—participants in this challenge will develop and benchmark new methods capable of gleaning actionable insights under real-time or near real-time constraints.</p> </blockquote> <p>In other word, AI City Challenge is an annual workshop held in order to promote the application of AI in real-world settings. This year (2025), there are 4 tracks available which are</p> <ul> <li>Track 1. Multi-Camera 3D Perception</li> <li>Track 2. Traffic Safety Description and Analysis</li> <li>Track 3. Warehouse Spatial Intelligence</li> <li>Track 4. Road Object Detection in Fish-Eye Cameras</li> </ul> <p>Our team participate in <strong>track 2</strong>, which requires captioning and answering questions related to traffic scenario. Track 2’s offciial description:</p> <blockquote> <p>Challenge Track 2. Traffic Safety Description and Analysis: Using multiple cameras and viewpoints, participants are challenged to describe both the moments leading up to incidents and the normal traffic flow, capturing all relevant details about pedestrian and vehicle behavior. The task also includes a video question-answering component to assess fine-grained understanding. The dataset has been enhanced with 3D gaze annotations and traffic video question answering. Quantitative scores will reflect accuracy on question answering, caption quality, and the fidelity of scene reconstruction in terms of directions, actions, and attributes.</p> </blockquote> <h2 id="datasets">Datasets</h2> <p>The provided datasets are <strong>WTS</strong> and <strong>BDD-PC-5K</strong> (filtered from BDD100K). Both dataset include videos and annotations related to the scenarios. In each scenario, there are multiple cameras located in different locations to provide multi-view about the scenario. Captions are provided and divided into 5 phases, from the moment pedestrian do not recognize the vehicle in phase 1, to the end of simluated accident in phase 5. The multiple choice questions cover a wide range of information such as pedestrian’ age group, direction of travel or environmental context. Other annotations such as bounding boxes for both pedestrian and vehicles are included, together with pedestrian’s gaze direction information.</p> <p>To be specific, the WTS dataset consists of 249 distinct scenarios and BDD consists of 3402 videos recorded using a vehicle’s dashboard camera. The average duration of</p> <h2 id="ideas">Ideas</h2> <p>In order to have a foundation to based on, we read previous works on the problem and extracting key ideas from the papers. After researching and reading a lot of papers on video captioning, traffic-related methods,… we decided our first main contribution is <strong>caption decomposition strategy</strong> (more detail can be read in paper). Derived from previous work, we utilized LLM such as Qwen-72B, to split the scenario’s descriptions into two parts, spatial (remain unchanged through time) and temporal (rapidly changed through time) instead of 4 parts (Appearance, Location, Environment and Attention). In this way, we <strong>reduce computational complexity required</strong> by using only 3 models instead of 5 models.</p> <p>The next question is how can we integrate temporal information to the model. This is when we introduce novel <strong>frame selection with best-view filtering strategy</strong> to include only meaningful and related frames.</p> <h2 id="training-stage">Training stage</h2> <h2 id="validating-results">Validating results</h2> <h2 id="ablation-studies">Ablation studies</h2> <h2 id="analysis-and-conclusion">Analysis and Conclusion</h2> </article> <h2>References</h2> <div class="publications"> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Thành Đạt . al-folio theme </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>